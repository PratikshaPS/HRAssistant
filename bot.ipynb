{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from groq import Groq\n",
    "from langchain_groq import ChatGroq\n",
    "import joblib\n",
    "import os\n",
    "import nest_asyncio  # noqa: E402\n",
    "nest_asyncio.apply()\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import chardet\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "\n",
    "llamaparse_api_key = os.environ.get('LLAMA_CLOUD_API_KEY')\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "def load_or_parse_data():\n",
    "    data_file = \"./data/parsed_data.pkl\"\n",
    "\n",
    "    if os.path.exists(data_file):\n",
    "        # Load the parsed data from the file\n",
    "        parsed_data = joblib.load(data_file)\n",
    "    else:\n",
    "        # Perform the parsing step and store the result in llama_parse_documents\n",
    "        parsingInstructionUber10k = \"\"\"The provided document is a HR policies\n",
    "        of an organization.\n",
    "        Try to be precise while answering the questions\"\"\"\n",
    "        parser = LlamaParse(api_key=llamaparse_api_key,\n",
    "                            result_type=\"markdown\",\n",
    "                            parsing_instruction=parsingInstructionUber10k,\n",
    "                            max_timeout=5000,)\n",
    "        llama_parse_documents = parser.load_data(\"./data/HR_Policy_Manual_KFSLnew.pdf\")\n",
    "\n",
    "\n",
    "        # Save the parsed data to a file\n",
    "        print(\"Saving the parse results in .pkl format ..........\")\n",
    "        joblib.dump(llama_parse_documents, data_file)\n",
    "\n",
    "        # Set the parsed data to the variable\n",
    "        parsed_data = llama_parse_documents\n",
    "\n",
    "    return parsed_data\n",
    "\n",
    "def convert_to_utf8(file_path):\n",
    "    \"\"\"\n",
    "    Convert the given file to UTF-8 encoding if it's not already in that format.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "\n",
    "    # Detect file encoding using chardet\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding'] or 'utf-8'  # Default to utf-8 if detection fails\n",
    "    # print(encoding)\n",
    "\n",
    "    print(f\"Detected encoding: {encoding}\")\n",
    "\n",
    "    # Decode using the detected encoding and re-encode as utf-8\n",
    "    text = raw_data.decode(encoding)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    print(f\"File converted to UTF-8 successfully: {file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def create_vector_database():\n",
    "    \"\"\"\n",
    "    Creates a vector database using document loaders and embeddings.\n",
    "\n",
    "    This function loads urls,\n",
    "    splits the loaded documents into chunks, transforms them into embeddings using OllamaEmbeddings,\n",
    "    and finally persists the embeddings into a Chroma vector database.\n",
    "\n",
    "    \"\"\"\n",
    "    # Call the function to either load or parse the data\n",
    "    llama_parse_documents = load_or_parse_data()\n",
    "    # print(llama_parse_documents)\n",
    "\n",
    "    with open('data/output.md', 'a', encoding='utf-8', errors='ignore') as f:  # Open the file in append mode ('a')\n",
    "        for doc in llama_parse_documents:\n",
    "            f.write(doc.text + '\\n')\n",
    "\n",
    "    markdown_path = \"./data/output.md\"\n",
    "\n",
    "    convert_to_utf8(\"data/output.md\")\n",
    "\n",
    "    loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "\n",
    "   #loader = DirectoryLoader('data/', glob=\"**/*.md\", show_progress=True)\n",
    "    documents = loader.load()\n",
    "    # Split loaded documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    #len(docs)\n",
    "    print(f\"length of documents loaded: {len(documents)}\")\n",
    "    print(f\"total number of document chunks generated :{len(docs)}\")\n",
    "    #docs[0]\n",
    "\n",
    "    # Initialize Embeddings\n",
    "    embed_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "    # Create and persist a Chroma vector database from the chunked documents\n",
    "    vs = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embed_model,\n",
    "        persist_directory=\"chroma_db_llamaparse1\",  # Local mode with in-memory storage only\n",
    "        collection_name=\"rag\"\n",
    "    )\n",
    "\n",
    "    print('Vector DB created successfully !')\n",
    "    return vs,embed_model\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name = 'gpt-3.5-turbo',\n",
    "    temperature = 0\n",
    "    )\n",
    "\n",
    "vs,embed_model = create_vector_database()\n",
    "\n",
    "def instantiate_vectordb(embed_model):\n",
    "    vectorstore = Chroma(embedding_function=embed_model,\n",
    "                        persist_directory=\"chroma_db_llamaparse1\",\n",
    "                        collection_name=\"rag\")\n",
    "\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={'k': 1})\n",
    "    return retriever\n",
    "\n",
    "retriever = instantiate_vectordb(embed_model)\n",
    "\n",
    "def set_custom_prompt():\n",
    "\n",
    "    custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "\n",
    "    Only return the helpful answer below in a complete sentence and nothing else.\n",
    "    format the answers in bullets wherever required and prettify the text. Mention all the nested points that are truly required.\n",
    "    Helpful answer:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt\n",
    "\n",
    "prompt = set_custom_prompt()\n",
    "print(prompt)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='result', output_key='result', return_messages=True)\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        combine_docs_chain_kwargs={'prompt':prompt}\n",
    "    )\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                               chain_type=\"stuff\",\n",
    "                               retriever=retriever,\n",
    "                               return_source_documents=True,\n",
    "                               memory = memory,\n",
    "                               chain_type_kwargs={\"prompt\": prompt})\n",
    "\n",
    "def handle_userinput(user_question):\n",
    "    response = st.session_state.conversation({'question': user_question})\n",
    "    st.session_state.chat_history = response['chat_history']\n",
    "\n",
    "    # for i, message in enumerate(st.session_state.chat_history):\n",
    "    #     if i % 2 == 0:\n",
    "    #         st.write(user_template.replace(\n",
    "    #             \"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
    "    #     else:\n",
    "    #         st.write(bot_template.replace(\n",
    "    #             \"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
    "            \n",
    "\n",
    "def main():\n",
    "    load_dotenv()\n",
    "\n",
    "    st.set_page_config(page_title=\"HR Assistant\",page_icon=\":books:\")\n",
    "    # st.write(css, unsafe_allow_html=True)\n",
    "\n",
    "    st.subheader(\"HR Assistant :books:\")\n",
    "\n",
    "    if \"conversation\" not in st.session_state:\n",
    "        st.session_state.conversation = None\n",
    "    if \"chat_history\" not in st.session_state:\n",
    "        st.session_state.chat_history = None\n",
    "\n",
    "    user_question = st.chat_input(\"Ask anything here:\")\n",
    "    if user_question:\n",
    "        handle_userinput(user_question)\n",
    "\n",
    "    st.session_state.conversation = conversation_chain\n",
    "\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
